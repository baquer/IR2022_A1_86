{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2a45535b",
      "metadata": {
        "id": "2a45535b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "08857f92-4efd-4438-cdae-4ba7d0c2db77"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import re as regex\n",
        "import scipy\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDGbvhe9NBg5",
        "outputId": "d423b7f9-db90-45d5-e355-8014bbfb2c8f"
      },
      "id": "VDGbvhe9NBg5",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6b942b3f",
      "metadata": {
        "id": "6b942b3f"
      },
      "outputs": [],
      "source": [
        "def getFilePath():\n",
        "  files = []\n",
        "  path = '/content/drive/MyDrive/Assignment-1-IR-F'\n",
        "  for d in glob.glob(os.path.join(path, '*')):\n",
        "    files.append(d)\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "-7v1x-UrOMVi"
      },
      "id": "-7v1x-UrOMVi",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowerCase(text):\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "axhheowjOol3"
      },
      "id": "axhheowjOol3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(text):\n",
        "  lemma = lemmatizer.lemmatize(text)\n",
        "  return lemma"
      ],
      "metadata": {
        "id": "NAz0Dd-HO_-J"
      },
      "id": "NAz0Dd-HO_-J",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopWord(text):\n",
        "  sentence = []\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  for w in range(0, len(text)):\n",
        "        if text[w] not in stop_words:\n",
        "            sentence.append(text[w])\n",
        "        else:\n",
        "          continue\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "DTy3fOmTVetf"
      },
      "id": "DTy3fOmTVetf",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f6e39c4f",
      "metadata": {
        "id": "f6e39c4f"
      },
      "outputs": [],
      "source": [
        "def positionalIndex():\n",
        "  terms = {}\n",
        "  files = getFilePath()\n",
        "\n",
        "  for i in range(len(files)):\n",
        "     # File Path \n",
        "      path = files[i]\n",
        "        \n",
        "    # Word Position\n",
        "      wordPos = 0\n",
        "\n",
        "    # Document ID\n",
        "      docId = files[i]\n",
        "\n",
        "    # Open the Current File\n",
        "\n",
        "      text = open(path,'r',errors='ignore').read()\n",
        "        \n",
        "      tokens = nltk.word_tokenize(text)\n",
        "      tokens = [item for item in tokens if item.isalpha()]\n",
        "      tokens = stopWord(tokens)\n",
        "\n",
        "      for item in tokens:\n",
        "        item = lowerCase(item)\n",
        "        lemma = lemmatization(item)\n",
        "\n",
        "        termKeys = terms.keys()             \n",
        "\n",
        "        if lemma in termKeys :\n",
        "\n",
        "          #Find the key of the token after lemmatization\n",
        "          newDict = terms[lemma]\n",
        "          newDictKeys = terms[lemma].keys()\n",
        "\n",
        "          #check if docId is present then append the word pos        \n",
        "          if docId in newDictKeys:\n",
        "            newList = newDict[docId]\n",
        "            newList.append(wordPos)\n",
        "            terms[lemma][docId] = newList\n",
        "\n",
        "          # if not present then make a new lsit\n",
        "          # and append the wordPos in list and then in dictionary\n",
        "          # corresponding to its doc name         \n",
        "          else:\n",
        "            newList = []\n",
        "            newList.append(wordPos)\n",
        "            terms[lemma][docId] = newList\n",
        "                    \n",
        "        # If not present then make a new dict and \n",
        "        # append a word pos corresponding to its key            \n",
        "        else:\n",
        "          makeNewList = []\n",
        "          makeNewDict = {}\n",
        "          makeNewList.append(wordPos)\n",
        "          makeNewDict[docId] = makeNewList\n",
        "          terms[lemma] = makeNewDict\n",
        "            \n",
        "        wordPos = wordPos + 1\n",
        "  return terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc339a70",
      "metadata": {
        "id": "fc339a70"
      },
      "outputs": [],
      "source": [
        "def executeQuery(inputquery):\n",
        "  terms = positionalIndex()\n",
        "  lemma = []\n",
        "  finalDict = {}\n",
        "  count = 0\n",
        "  # Preprocessing\n",
        "  tokens = nltk.word_tokenize(inputquery)\n",
        "  tokens = [item for item in tokens if item.isalpha()]\n",
        "  tokens = stopWord(tokens)\n",
        "  for item in tokens:\n",
        "    print(item)\n",
        "    item = item.lower()\n",
        "    lemma += [lemmatizer.lemmatize(item)]\n",
        "  for item in lemma :\n",
        "    print(item)\n",
        "    newDict = terms[item]\n",
        "    if(len(finalDict) > 0):\n",
        "        tempDict = {}\n",
        "        documentList = list(set(finalDict.keys()) & set(newDict.keys())) \n",
        "        if len(documentList) == 0:\n",
        "            print(\"No Documety\")\n",
        "            break\n",
        "        # store doc in a dict and finally store in a final dict\n",
        "        for doc in documentList :\n",
        "            tempDict[doc] = finalDict[doc]     \n",
        "        finalDict = tempDict\n",
        "        # Again Iterate and find the position \n",
        "        for doc in documentList:\n",
        "            \n",
        "            list_posting = finalDict[doc]\n",
        "            temp_pos = []\n",
        "            \n",
        "            for pos in list_posting:\n",
        "                \n",
        "                if pos+count in newDict[doc]:\n",
        "                    temp_pos.append(pos)\n",
        "            \n",
        "            finalDict[doc] = temp_pos\n",
        "            \n",
        "            if len(finalDict[doc]) == 0 :\n",
        "                del finalDict[doc]\n",
        "    else:\n",
        "        finalDict = newDict\n",
        "    count += 1  \n",
        "  return finalDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = str(input('Enter Query:'))\n",
        "finalResult = executeQuery(query)\n",
        "print(\"Number of Documents Retrived are as Follows:\" ,len(finalResult))\n",
        "finalResult"
      ],
      "metadata": {
        "id": "gXVHMm693kF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d47778de-3551-4652-ecaa-5b692f0bfd07"
      },
      "id": "gXVHMm693kF2",
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Query:Education \n",
            "Education\n",
            "education\n",
            "Number of Documents Retrived are as Follows: 47\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/drive/MyDrive/Assignment-1-IR-F/acronyms.txt': [1649,\n",
              "  3360,\n",
              "  4812,\n",
              "  5579,\n",
              "  6181,\n",
              "  7203],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/arnold.txt': [503],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/basehead.txt': [1601],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/bnbeg2.4.txt': [1105],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/bnbguide.txt': [1047],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/calvin.txt': [450, 476],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/childhoo.jok': [399],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/clancy.txt': [804],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/collected_quotes.txt': [1374],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/commword.hum': [367, 2442],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/critic.txt': [2458],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/cybrtrsh.txt': [1222],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/devils.jok': [285],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/dubltalk.jok': [233],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/econridl.fun': [622],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/epi_bnb.txt': [1035],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/films_gl.txt': [2863],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/hack7.txt': [3877],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/humor9.txt': [4318,\n",
              "  7944,\n",
              "  7963,\n",
              "  14268],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/insult.lst': [3079,\n",
              "  8047,\n",
              "  8672,\n",
              "  10289],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/insults1.txt': [2521, 2974],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/japantv.txt': [1157,\n",
              "  1159,\n",
              "  1163,\n",
              "  1174],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/jason.fun': [418],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/kloo.txt': [903],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/limerick.jok': [534],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/llong.hum': [1113],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/losers84.hum': [173,\n",
              "  257,\n",
              "  353,\n",
              "  484,\n",
              "  554,\n",
              "  808,\n",
              "  1042,\n",
              "  1277,\n",
              "  1541,\n",
              "  1767,\n",
              "  1918,\n",
              "  2034],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/macsfarm.old': [36],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/manners.txt': [1567],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/nosuch_nasfic': [509],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/onetoone.hum': [800],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/onetotwo.hum': [5572],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/packard.txt': [474],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/parabl.hum': [131],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/passage.hum': [1142],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/phunatdi.ana': [288, 338],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/poll2res.hum': [56, 106, 119, 158],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/prac4.jok': [1835],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/practica.txt': [16634],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/prawblim.hum': [41],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/socecon.hum': [5],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/social.hum': [5],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/strine.txt': [1296],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/tnd.1': [1284, 1306],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/top10.txt': [797],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/wrdnws2.txt': [278],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/wrdnws3.txt': [280, 326]}"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "IR_A1_Q2_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}