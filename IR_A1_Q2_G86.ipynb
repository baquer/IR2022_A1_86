{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2a45535b",
      "metadata": {
        "id": "2a45535b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b9df604a-3ebb-461b-c986-1c7adb3962cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import os\n",
        "import glob\n",
        "import re as regex\n",
        "import scipy\n",
        "from collections import Counter\n",
        "from nltk.corpus import stopwords \n",
        "from nltk.tokenize import word_tokenize,RegexpTokenizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import wordpunct_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem import WordNetLemmatizer \n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "ps = PorterStemmer()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VDGbvhe9NBg5",
        "outputId": "3eee3446-4bfe-4441-f7a9-19c3a5a258b1"
      },
      "id": "VDGbvhe9NBg5",
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "6b942b3f",
      "metadata": {
        "id": "6b942b3f"
      },
      "outputs": [],
      "source": [
        "def getFilePath():\n",
        "  files = []\n",
        "  path = '/content/drive/MyDrive/Assignment-1-IR-F'\n",
        "  for d in glob.glob(os.path.join(path, '*')):\n",
        "    files.append(d)\n",
        "  return files"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenize(text):\n",
        "  tokens = nltk.word_tokenize(text)\n",
        "  return tokens"
      ],
      "metadata": {
        "id": "-7v1x-UrOMVi"
      },
      "id": "-7v1x-UrOMVi",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lowerCase(text):\n",
        "  text = text.lower()\n",
        "  return text"
      ],
      "metadata": {
        "id": "axhheowjOol3"
      },
      "id": "axhheowjOol3",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def lemmatization(text):\n",
        "  lemma = lemmatizer.lemmatize(text)\n",
        "  return lemma"
      ],
      "metadata": {
        "id": "NAz0Dd-HO_-J"
      },
      "id": "NAz0Dd-HO_-J",
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def stopWord(text):\n",
        "  sentence = []\n",
        "  stop_words = set(stopwords.words(\"english\"))\n",
        "  for w in range(0, len(text)):\n",
        "        if text[w] not in stop_words:\n",
        "            sentence.append(text[w])\n",
        "        else:\n",
        "          continue\n",
        "  return sentence"
      ],
      "metadata": {
        "id": "DTy3fOmTVetf"
      },
      "id": "DTy3fOmTVetf",
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "f6e39c4f",
      "metadata": {
        "id": "f6e39c4f"
      },
      "outputs": [],
      "source": [
        "def positionalIndex():\n",
        "  terms = {}\n",
        "  files = getFilePath()\n",
        "\n",
        "  for i in range(len(files)):\n",
        "     # File Path \n",
        "      path = files[i]\n",
        "        \n",
        "    # Word Position\n",
        "      wordPos = 0\n",
        "\n",
        "    # Document ID\n",
        "      docId = files[i]\n",
        "\n",
        "    # Open the Current File\n",
        "\n",
        "      text = open(path,'r',errors='ignore').read()\n",
        "        \n",
        "      tokens = nltk.word_tokenize(text)\n",
        "      tokens = [item for item in tokens if item.isalpha()]\n",
        "      tokens = stopWord(tokens)\n",
        "\n",
        "      for item in tokens:\n",
        "        item = lowerCase(item)\n",
        "        lemma = lemmatization(item)\n",
        "\n",
        "        termKeys = terms.keys()             \n",
        "\n",
        "        if lemma in termKeys :\n",
        "\n",
        "          #Find the key of the token after lemmatization\n",
        "          newDict = terms[lemma]\n",
        "          newDictKeys = terms[lemma].keys()\n",
        "\n",
        "          #check if docId is present then append the word pos        \n",
        "          if docId in newDictKeys:\n",
        "            newList = newDict[docId]\n",
        "            newList.append(wordPos)\n",
        "            terms[lemma][docId] = newList\n",
        "\n",
        "          # if not present then make a new lsit\n",
        "          # and append the wordPos in list and then in dictionary\n",
        "          # corresponding to its doc name         \n",
        "          else:\n",
        "            newList = []\n",
        "            newList.append(wordPos)\n",
        "            terms[lemma][docId] = newList\n",
        "                    \n",
        "        # If not present then make a new dict and \n",
        "        # append a word pos corresponding to its key            \n",
        "        else:\n",
        "          makeNewList = []\n",
        "          makeNewDict = {}\n",
        "          makeNewList.append(wordPos)\n",
        "          makeNewDict[docId] = makeNewList\n",
        "          terms[lemma] = makeNewDict\n",
        "            \n",
        "        wordPos = wordPos + 1\n",
        "  return terms"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "fc339a70",
      "metadata": {
        "id": "fc339a70"
      },
      "outputs": [],
      "source": [
        "def executeQuery(inputquery):\n",
        "  terms = positionalIndex()\n",
        "  lemma = []\n",
        "  finalDict = {}\n",
        "  count = 0\n",
        "  # Preprocessing\n",
        "  tokens = nltk.word_tokenize(inputquery)\n",
        "  tokens = [item for item in tokens if item.isalpha()]\n",
        "  tokens = stopWord(tokens)\n",
        "  for item in tokens:\n",
        "    print(item)\n",
        "    item = lowerCase(item)\n",
        "    lemma += [lemmatization(item)]\n",
        "  for item in lemma :\n",
        "    print(item)\n",
        "    newDict = terms[item]\n",
        "    if(len(finalDict) > 0):\n",
        "        tempDict = {}\n",
        "        documentList = list(set(finalDict.keys()) & set(newDict.keys())) \n",
        "        if len(documentList) == 0:\n",
        "            print(\"No Documet Found\")\n",
        "            break\n",
        "        # store doc in a dict and finally store in a final dict\n",
        "        for document in documentList :\n",
        "            tempDict[document] = finalDict[document]     \n",
        "        finalDict = tempDict\n",
        "        # Again Iterate and find the position \n",
        "        for doc in documentList:\n",
        "            \n",
        "            list_posting = finalDict[doc]\n",
        "            temp_pos = []\n",
        "            \n",
        "            for pos in list_posting:\n",
        "                \n",
        "                if pos+count in newDict[doc]:\n",
        "                    temp_pos.append(pos)\n",
        "            \n",
        "            finalDict[doc] = temp_pos\n",
        "            \n",
        "            if len(finalDict[doc]) == 0 :\n",
        "                del finalDict[doc]\n",
        "    else:\n",
        "        finalDict = newDict\n",
        "    count += 1  \n",
        "  return finalDict"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "query = str(input('Enter Query:'))\n",
        "finalResult = executeQuery(query)\n",
        "print(\"Number of Documents Retrived are as Follows:\" ,len(finalResult))\n",
        "finalResult"
      ],
      "metadata": {
        "id": "gXVHMm693kF2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "669e38f3-9ba2-449e-9cd4-c19a810bc59d"
      },
      "id": "gXVHMm693kF2",
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter Query:lion \n",
            "lion\n",
            "lion\n",
            "Number of Documents Retrived are as Follows: 27\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'/content/drive/MyDrive/Assignment-1-IR-F/bitnet.txt': [1079],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/boneles2.txt': [121,\n",
              "  129,\n",
              "  142,\n",
              "  792,\n",
              "  1255,\n",
              "  1258,\n",
              "  1330,\n",
              "  1337,\n",
              "  1344,\n",
              "  1374,\n",
              "  1405,\n",
              "  1428,\n",
              "  1583],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/booze1.fun': [693],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/christop.int': [1021],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/collected_quotes.txt': [3650],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/computer.txt': [204],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/deep.txt': [593],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/dromes.txt': [491],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/drunk.txt': [73],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/dthought.txt': [608],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/filmgoof.txt': [5654],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/hecomes.jok': [245],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/japantv.txt': [360, 444],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/lion.jok': [18,\n",
              "  44,\n",
              "  79,\n",
              "  104,\n",
              "  110,\n",
              "  113,\n",
              "  129,\n",
              "  149,\n",
              "  176,\n",
              "  189,\n",
              "  205,\n",
              "  233,\n",
              "  257,\n",
              "  262,\n",
              "  295,\n",
              "  305,\n",
              "  321,\n",
              "  356,\n",
              "  363,\n",
              "  373,\n",
              "  383,\n",
              "  404,\n",
              "  411,\n",
              "  418,\n",
              "  428,\n",
              "  438,\n",
              "  445,\n",
              "  456,\n",
              "  461,\n",
              "  475,\n",
              "  494,\n",
              "  511,\n",
              "  527,\n",
              "  538,\n",
              "  589,\n",
              "  591,\n",
              "  598,\n",
              "  637,\n",
              "  669,\n",
              "  720,\n",
              "  744,\n",
              "  761,\n",
              "  766,\n",
              "  794,\n",
              "  816,\n",
              "  820,\n",
              "  839,\n",
              "  851,\n",
              "  856,\n",
              "  889,\n",
              "  903,\n",
              "  908,\n",
              "  934],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/lion.txt': [10,\n",
              "  35,\n",
              "  41,\n",
              "  44,\n",
              "  60,\n",
              "  80,\n",
              "  107,\n",
              "  120,\n",
              "  136,\n",
              "  164,\n",
              "  188,\n",
              "  193,\n",
              "  226,\n",
              "  236,\n",
              "  252,\n",
              "  287,\n",
              "  294,\n",
              "  304,\n",
              "  314,\n",
              "  335,\n",
              "  342,\n",
              "  349,\n",
              "  359,\n",
              "  369,\n",
              "  376,\n",
              "  387,\n",
              "  392,\n",
              "  406,\n",
              "  425,\n",
              "  442,\n",
              "  458,\n",
              "  469,\n",
              "  520,\n",
              "  522,\n",
              "  529,\n",
              "  568,\n",
              "  600,\n",
              "  651,\n",
              "  675,\n",
              "  692,\n",
              "  697,\n",
              "  725,\n",
              "  747,\n",
              "  751,\n",
              "  770,\n",
              "  782,\n",
              "  787,\n",
              "  820,\n",
              "  834,\n",
              "  839,\n",
              "  865],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/lions.cat': [85,\n",
              "  110,\n",
              "  116,\n",
              "  119,\n",
              "  135,\n",
              "  155,\n",
              "  182,\n",
              "  195,\n",
              "  211,\n",
              "  239,\n",
              "  263,\n",
              "  268,\n",
              "  301,\n",
              "  311,\n",
              "  327,\n",
              "  362,\n",
              "  369,\n",
              "  379,\n",
              "  389,\n",
              "  410,\n",
              "  417,\n",
              "  424,\n",
              "  434,\n",
              "  444,\n",
              "  451,\n",
              "  463,\n",
              "  468,\n",
              "  482,\n",
              "  501,\n",
              "  518,\n",
              "  534,\n",
              "  545,\n",
              "  596,\n",
              "  598,\n",
              "  605,\n",
              "  644,\n",
              "  702,\n",
              "  732,\n",
              "  754,\n",
              "  758,\n",
              "  776,\n",
              "  788,\n",
              "  793,\n",
              "  826,\n",
              "  840,\n",
              "  845,\n",
              "  871,\n",
              "  935,\n",
              "  965,\n",
              "  987,\n",
              "  991,\n",
              "  1009,\n",
              "  1021,\n",
              "  1026,\n",
              "  1059,\n",
              "  1073,\n",
              "  1078,\n",
              "  1104],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/llong.hum': [621, 625],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/mindvox': [275],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/murphy_l.txt': [1340],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/murphys.txt': [1333],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/onetotwo.hum': [3347],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/puzzles.jok': [102],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/stuf10.txt': [530, 1749, 1821],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/three.txt': [20],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/tnd.1': [583],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/tpquotes.txt': [1098],\n",
              " '/content/drive/MyDrive/Assignment-1-IR-F/wagon.hum': [75]}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "IR_A1_Q2_.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}